{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pharma-gov-scrape.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1qY9zjKjxCsVzR7zdZ-Q8IwO2MQiNQakE",
      "authorship_tag": "ABX9TyPd57x3sMgFXARUmub3U3XD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/exglade/pharma-gov-scrape/blob/main/pharma_gov_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79PM9pgbEd3P"
      },
      "source": [
        "# Scrape Pharmaceutical Products Data from Government"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-wKaTLwEusb"
      },
      "source": [
        "I wants to scrap the database from the Quest system:\n",
        "\n",
        "https://npra.gov.my/index.php/en/consumers/information/products-search.html\n",
        "\n",
        "Last known in 2017, there was about ~25k pharmaceutical products and ~13k cosmetic products.\n",
        "\n",
        "It would also be useful to extract all drugs from https://www.pharmacy.gov.my/v2/en/apps/fukkm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od5ShHDqLyzm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WceN65ruL4Br"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/pharma-gov-scrape/Data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMLJXlbYFImF"
      },
      "source": [
        "## Scrape NPRA's Quest 3+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJKxjWQBFSKC"
      },
      "source": [
        "**Target:** https://npra.gov.my/index.php/en/consumers/information/products-search.html\n",
        "\n",
        "The target page has a search form with following fields:\n",
        "\n",
        "- **Product category**: indicate to search pharmaceutical or cosmetic products.\n",
        "- **Search by**: the product attribute to search by.\n",
        "- **Search**: the text to query, minimum 5 characters.\n",
        "\n",
        "The search form call the API at `https://quest3plus.bpfk.gov.my/pmo2/content.php` with following payload schema:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'func': 'search', # API's function name?\n",
        "    'searchBy': '1', # The 'Search by' field.\n",
        "    'searchTxt': '', # The 'Search' field\n",
        "    'cat': '2' # The 'Product category' field, where 1 = Pharmaceutical, 2 = Cosmetic\n",
        "}\n",
        "```\n",
        "\n",
        "The API returns the search result as partial HTML in `<table>` form. The page will paginate the search result using client-side JavaScript.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- API has no minimum character limit on `searchTxt`. We can send blank text.\n",
        "- API has no max result on query. We can get all results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk49lCtUEdVq"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_quest(category):\n",
        "  data = {\n",
        "      'func': 'search',\n",
        "      'searchBy': '1',\n",
        "      'searchTxt': '',\n",
        "      'cat': category\n",
        "  }\n",
        "\n",
        "  url = 'https://quest3plus.bpfk.gov.my/pmo2/content.php'\n",
        "  res = requests.post(url, data=data, verify=False)\n",
        "  soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "  rows = soup.select('tr')[1:]\n",
        "\n",
        "  output = []\n",
        "  for row in rows:\n",
        "    cols = row.select('td')\n",
        "    output_cols = []\n",
        "    for col in cols:\n",
        "      output_cols.append(col.text.strip())\n",
        "    output.append(output_cols)\n",
        "\n",
        "  print(f\"Query '{url}' with category: {category}, found {len(output)} results\")\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdrDdyFCNEY6"
      },
      "source": [
        "import csv\n",
        "\n",
        "def save_npra_result(filepath, results):\n",
        "  with open(filepath, mode='w') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(['#', 'Registration Number', 'Product Name', 'Holder'])\n",
        "    for result in results:\n",
        "      writer.writerow(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDPSObYsOgdA"
      },
      "source": [
        "# Pharmaceutical\n",
        "pharma_prods = scrape_quest('1')\n",
        "save_npra_result(data_dir + 'pharma-prods.csv', pharma_prods)\n",
        "\n",
        "# Cosmetics\n",
        "cosmet_prods = scrape_quest('2')\n",
        "save_npra_result(data_dir + 'cosmet-prods.csv', cosmet_prods)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPous-I6FqIU"
      },
      "source": [
        "## Scrape Formulari Ubat KKM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_nVwpxZFyIB"
      },
      "source": [
        "**Target:** https://www.pharmacy.gov.my/v2/en/apps/fukkm\n",
        "\n",
        "The target page shows 30 items in a standard HTML table per page. There are a total of 55 pages with a total of 1639 items as of 26/4/2021.\n",
        "\n",
        "The page accepts a query parameter for the page number (zero-based): `https://www.pharmacy.gov.my/v2/en/apps/fukkm?page={page_number}`\n",
        "\n",
        "The table has following columns:\n",
        "\n",
        "- #\n",
        "- Generic Name\n",
        "- MDC\n",
        "- Category\n",
        "- Indications\n",
        "- Pres. Restrictions\n",
        "- Dosage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnCBmFYvPApZ"
      },
      "source": [
        "# Scrape FUKKM function\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_fukkm(page):\n",
        "  output = []\n",
        "  url = f\"https://www.pharmacy.gov.my/v2/en/apps/fukkm?page={page}\"\n",
        "  res = requests.get(url)\n",
        "\n",
        "  soup = BeautifulSoup(res.content, 'html.parser')\n",
        "  rows = soup.select('tr')[1:]\n",
        "\n",
        "  for row in rows:\n",
        "    cols = row.select('td')\n",
        "    output_cols = []\n",
        "    for col in cols:\n",
        "      output_cols.append(col.text.strip())\n",
        "    output.append(output_cols)\n",
        "\n",
        "  print(f\"Query '{url}', found {len(output)} results\")\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiFN4KPtPNC4"
      },
      "source": [
        "# Run FUKKM scraping\n",
        "\n",
        "from concurrent.futures.thread import ThreadPoolExecutor\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=20) as executor:\n",
        "  results = executor.map(scrape_fukkm, range(0,55)) # FUKKM has 55 pages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2rEQoO-PB3P"
      },
      "source": [
        "# Save FUKKM results to file\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(data_dir + 'fukkm-output.csv', mode='w') as csv_file:\n",
        "  writer = csv.writer(csv_file)\n",
        "  writer.writerow([\"#\", \"Generic Name\", \"MDC\", \"Category\",\"Indications\",\"Pres. Restrictions\",\"Dosage\"])\n",
        "  for result in results:\n",
        "    writer.writerow(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}